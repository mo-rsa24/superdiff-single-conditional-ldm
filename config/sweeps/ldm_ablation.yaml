# config/sweeps/ldm_ablation.yaml
# Run from the repository root: wandb sweep config/sweeps/ldm_ablation.yaml

program: train_ldm.py  # Assuming you rename run/ldm.py to train_ldm.py
method: grid           # Use Bayesian for exploration, Grid for checking fixed values
metric:
  name: sample_diversity/pairwise_mse_mean # Use this as the main quality metric
  goal: maximize                           # Maximize diversity

parameters:
  # Architecture (Supervisor's Focus C)
  ldm_base_ch:
    values: [64, 128]                      # Base channels
  ldm_ch_mults:
    values: ["1,2,4,4", "1,2,4"]           # Depth/multipliers
  ldm_num_res_blocks:
    values: [1, 2]                         # Residual blocks per level
  ldm_attn_res:
    values: ["16", "16,8"]                 # Attention resolutions

  # Optimization
  lr:
    values: [1.0e-4, 5.0e-5]               # Learning rate
  use_ema:
    values: [True, False]                  # EMA vs non-EMA
  ema_decay:
    values: [0.999, 0.99]                  # EMA decay rate (if used)

  # CFG/Data (Supervisor's Focus C)
  prob_uncond:
    values: [0.1, 0.2]                     # Label dropout for CFG
  guidance_scale:
    values: [3.0, 5.0]                     # Sampling guidance scale
  latent_scale_factor:
    values: [1.0, 0.18215]                 # VAE latent scaling

  # Sampling (Supervisor's Focus A)
  # NOTE: Add a command-line argument for sampling steps in train_ldm.py
  num_sampling_steps:
    values: [100, 500]                     # Number of steps for the Euler Sampler

# Fixed arguments for the specific dataset/run:
command:
  - python
  - ${program}
  - ${args}
  - --wandb
  - --data_root
  - "/datasets/mmolefe/cleaned/"
  - --ae_ckpt_path
  - "/path/to/vae_ckpt/last.flax"
  - --ae_config_path
  - "/path/to/vae_ckpt/run_meta.json"